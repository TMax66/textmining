---
title: "Text mining analysis"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
####TEXT MINING

Con il termine text mining si intende un "processo di distillazione di informazioni utili da un testo". 
Si tratta di un insieme di metodiche quantitative che utilizzano come "unità" d'analisi i termini ( parole) presenti in un testo. Si applica a diversi tipi di testi: libri, tweet, mail, survey, ecc.. qualsiasi testo può difatti essere analizzato.

In sostanza  il testo viene dapprima "tokenizzato" cioè ridotto ad una sequenza di semplici  termini privato di quelle parole o vocaboli che servono alla definizione sensata  e comprensibile di un periodo. Queste documento viene quindi trasformato in una Term Document Matrix, cioè una matrice che riporta per ogni singolo termine quante volte compare in un singolo documento. Da questa matrice si ottengono poi tutti i tipi di analisi testuali tra cui: frequenza delle parole, wordcloud associazione tra parole, analisi dei cluster, topic model analysis, sentiment analysis e molte altre...

Il testo su cui stiamo lavorando è il risultato di un Survey in cui sono state date delle risposte aperte a definite domande.
Ogni singola risposta viene definita come 'documento'. 
Per poter essere analizzate queste domande, si costruisce prima un cosidetto Corpus che altro non è che una matrice con due colonne. La prima colonna definisce il numero del documento e la seconda riporta il testo del documento. 

Dal Corpus si ricava la Term Document Matrix, dopo aver tolto le parole non utili dette stop words,la punteggiatura, i numeri,ecc..
Di seguito le prime 10 colonne e righe del Corpus ricavato dalla domanda 10 del questionario dei cavalli. La TDM realtiva alla domanda 10 ha 242 righe (termini) e 122 colonne (documenti/risposte).

```{r echo=FALSE}
tdm.q10.m[1:10,1:10]
```

##### 1.Looking at your neighbor's horse, what signs would you identify, to evaluate the conditions of accommodation:


Word Frequency and Word Clouds

Questo tipo di analisi è il più semplice è riporta in forma di visualizzazione la "frequenza " di comparsa nel Corpus dei singoli termini . In ambito di text mining le parole che compaiono più frequentemente sono considerate le più importanti.

La WordCLoud è un modo efficace di visualizzare l'importanza delle parole in un Corpus in relazione alla loro frequenza.

```{r echo=FALSE}
ggplot(freq.df[1:20,], aes(x=word, y=frequency))+geom_bar(stat = "identity", fill='darkred')+
  coord_flip()+theme_gdocs()+geom_text(aes(label=frequency), colour="white",hjust=1.25, size=5.0)
```

```{r echo=FALSE}
set.seed(500)
wordcloud(freq.df$word, freq.df$frequency, max.words=100, colors=c('black','darkred'))
```

###Network of terms 

Questo grafico mostra la rete tra parole in termini di 'associazione'. In sostanza per ogni singola risposta valuta la coopresenza dei termini ( tutte le possibili combinazioni) e traccia una rete.  I criteri utilizzati per definire questa rete sono:
-termini che appaiono con una frequenza maggiore o uguale a 10
- grado di associazione (correlazione) maggiore o uguale a 0.20 

Un network è costituito da "Vertici", che nel nostro caso sono rappresentati dai termini e da "edge" cioè linee che uniscono i vari vertici indicando la presenza di una coopresenza. Il differente spessore delle linee è proporzionale al grado di associazione (vedi dopo) tra i termini: maggiore è lo spessore maggiore è l'associazione/copresenza tra i due termini.




```{r echo=FALSE}
freq.term<-findFreqTerms(tdm, lowfreq = 10)

plot(tdm, term=freq.term, corThreshold = 0.2,weighting=T)

```

### Associazione tra termini

In text mining il termine associazione/correlazione va inteso come co-presenza di termini nello stesso documento (nel nostro caso il documento è la singola risposta alla domanda, quindi abbiamo per i cavalli 121 risposte cioè 121 documenti). L'analisi di associazione o copresenza misura quindi,   la copresenza tra un termine d'interesse definito da noi e tutte le altre parole. Usa una scala da 0 a 1 maggiore è il valore di associazione maggiore è la copresenza tra due termini... non esiste un modo per definire in termini assoluti l'importanza di una associazione, conta più il confronto... normalmente il minimo valore preso come soglia d'importanza è 0.20. 

 Ad esempio, dato che il termine Clean risulta il più frequente nel "Corpus" delle risposte potremmo essere interessati a esplorare con quali altri termini "clean" si riscontra in uno stesso documento.   Dal network dei termini si può già vedere che clean è associato con water e light, sicuramente questi due termini hanno una frequenza di comparsa >= a 10 e hanno un grado di associazione >= 0.20. Nel grafico che segue sono riportate le associazioni del termine clean con tutti i termini con cui ha un'associazione superiore a 0.20


```{r echo=FALSE}
ggplot(associations, aes(y=terms))+
  geom_point(aes(x=clean), data=associations, size=1)+
  theme_gdocs()+geom_text(aes(x=clean, label=clean),
                          colour="darkred", hjust=-.25, size=3)+
  theme(text=element_text(size=8),
        axis.title.y = element_blank())
```


Come si può vedere ci sono molti dei termini con cui "Clean" è associato, ma che  non compaiono nel Network.  Questo perchè termini come floors, manger, drinking...ecc  non hanno una frequenza di comparsa >= a 10. Se avessi abbassato la soglia di frequenza a 5 il Network sarebbe stato "illeggibile" perchè anche una sola co-presenza sarebbe comparsa nel grafico generenado una rete confusa di linee e rettangoli.

La copresenza è intesa nell'ambito dello stesso documento non nello stesso periodo o come sequenza di parole.

Questo di Clean è un esempio di analisi delle associazioni. Come detto va definito prima il termine d'interesse e poi si effettua l'analisi. Quindi dovreste indicarmi di quali termini siete interessati ad esplorare il grado di associazione. 


### Word Cluster

Questo approccio è un semplice sistema per esplorare visivamente le associazioni tra parole in un corpus. E' a tutti gli effetti un analisi dei cluster che utilizza la frequenza di comparsa delle parole come misura della distanza tra le parole stesse per identificare in modo gerarchico i raggruppamenti di parole. Lasciando perdere la parte teorica, dalla domanda 10 si ottiene il seguente dendrogramma:

```{r echo=FALSE, fig.height=10}
plot(hc, yaxt='n', main="", hang=0.5, cex=1.5)
```
L'interpretazione è abbastanza semplice. Il termine clean è un gruppo a sè. A causa della frequenza elevata in tutti documenti, praticamente fa "cluster" con tutte le altre parole.
Anche il termine "Presence" è un termine che è molto distante da tutte gli altri termini da risultare come un cluster a se, ma sinceramente non so darne una spiegazione...Non escluderei che si tratti di un termine che potrebbe finire tra le stop words, in quanto è un termine che sicuramente accompagna molti altri termini la cui "presenza" è più o meno importante relativamente alla domanda.
Poi cè un grande cluster fatto da tutti gli altri termini che tendono a raggrupparsi in ulteriori piccoli cluster che hanno un loro senso:
ad esempio c'è un cluster fatto dai termini: pasture,safe,fencing, rain, wind, sun, supply,equipment,grazing, che raccoglie termini associati alla condizione delle strutture/riparo condizioni ambientali, ec..

Dal dendogramma mi pare di vedere anche termini come "enough", "animals", "well"... che forse andrebbe valutato se inserirli o meno tra le stop words.



####Relazioni tra termini: n-gram analysis

Fino ad ora sono state analizzate singole parole/termini che hanno rappresentato l'unità di analisi. E' possibile, oltre che interessante,studiare gruppi di parole che appaiono in sequenza. Gruppi di 2, 3 , n parole. L'analisi per gruppi di due parole detti "bigrammi"" fornisce già di per sè utili informazioni rimanendo abbastanza intelleggibile, amumentando il numero dei termini in una sequenza i risultati si fanno complicati, e forse per il nostro lavoro non sono particolarmente indicati.

In questa tabella sono indicate le coppie di parole adiacenti e la freequenza di comparsa nelle risposte alla domanda 10.

```{r echo=FALSE}
q10bsepf[1:10,]
```

E' possibile costruire una rappresentazione grafica del network di "bigrammi" che sono presenti nel Corpus. Si ottiene questo grafico:



```{r echo=FALSE,fig.height=5,fig.width=5}
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

Le frecce indicano la direzione della sequenza dei termini. Ad esempio nel Corpus clean compare dopo ligth  e fresh ma prima di bedding e water. 

####TOPIC MODELING

Questo tipo di analisi permette di evidenziare la presenza nel corpus di differenti "topic", ovvero raggruppamenti di termini che descrivono argomenti differenti: cioè evidenzia raggruppamenti naturali di termini anche quando non siamo sicuri di quello che stiamo cercando. Da questo punto di vista è una tecnica esplorativa. Assegna ad ogni termine la probabilità di appartenere a differenti topic. I risultati possono essere visualizzati in questo modo ad esempio:


```{r echo=FALSE}
ap_top_terms <- q10topics %>%
  group_by(topic) %>%
  top_n(20, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  mutate(topic = paste0("topic", topic)) %>%
  ggplot(aes(term, beta)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

In questo tipo di analisi il numero di topic si stabilisce prima. Con il corpus della domanda 10 ho verificato come si ditribuiscono i termini in due topic principali. 
Osservando il grafico dovreste provare a dare un senso ai termini che caraterizzano i due topic in modo da poterlo poi definire. Non è detto che ci si riesca, ma se ci si riesce abbiamo una informazione importante. COme potete vedere Clean è omnipresente...quindi sicuramente non è caratterizzante. CI sono altri termini in comune tra i  due topic: questo non è uno svantaggio in quanto si ritiene che in questi ambiti differenti argomenti pssoo essere descritti dallo stesso termine che può assumere significati diversi. Quello su cui ci si dovrebbe concetrae sono i termini presenti solo in un topic e non nell'altro. Dal grafico risulta ad esempio che il topic1 si caratterizza per i termini:safe,fresh,area, access,appropriate,adequate,turn, stalls, light, nails. Riuscite a definire questo topic di cosa tratta? lo stesso per il topic 2 . Volendo possiamo anche esplorare un numero di topic maggiore. 


####Brevi considerazioni###
Quanto ho fatto finora è a titolo di esempio per la domanda q10. Credo che ci sia un limite derivante dalla scarsa numerosità dei termini, più che del numero di documenti (cioè risposte). Infatti ci sono molte risposte davvero molto brevi. Gli esempi che ho trovato su queste metodiche hanno sempre decine di migliaia di termini ( quindi le ricorrenze, le associazioni sono forse più frequenti).  Un Corpus usato come esempio aveva 10473 termini in 2246 documenti. La differenza è notevole. Quindi credo che il grosso del lavoro qui sia molto speculativo cercando di dare un senso a quanto indicato dalle varie analisi che possiamo fare.

